from sac3 import llm_models

class Evaluate:
    """
    A class to evaluate responses generated by language models (LLMs).
    It supports both the evaluation of direct responses to a question and
    responses to semantically perturbed versions of the question.
    """

    def __init__(self, model):
        """
        Initialize the Evaluate class.

        Args:
            model (str): The name or configuration of the language model to use for evaluation.
        """
        self.model = model
        
        # Template prompt to guide LLM in generating answers.
        self.prompt_temp = 'Answer the following question:\n'
    
    def self_evaluate(self, self_question, temperature, self_num):
        """
        Generate multiple responses to a single question.

        This method evaluates the consistency of responses generated by the model
        for the same question under varying randomness (temperature).

        Args:
            self_question (str): The original user query to evaluate.
            temperature (float): Sampling temperature to control randomness (0 = deterministic).
            self_num (int): Number of responses to generate for the question.

        Returns:
            list: Generated responses for the given question.
        """
        self_responses = []  # List to store generated responses
        prompt = self.prompt_temp + '\nQ:' + self_question  # Construct the question prompt
    
        # Generate multiple responses sequentially
        for i in range(self_num): 
            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                # Call OpenAI API for GPT models
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                # Call Guanaco-33B model API
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                # Call Falcon-7B model API
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")
            
            # Append each generated response to the list
            self_responses.append(res)

        return self_responses
    
    def perb_evaluate(self, perb_questions, temperature):
        """
        Generate responses to perturbed (semantically equivalent) questions.

        This method evaluates the consistency of responses generated by the model
        for multiple semantically equivalent versions of the original question.

        Args:
            perb_questions (list): List of perturbed questions.
            temperature (float): Sampling temperature to control randomness (0 = deterministic).

        Returns:
            list: Generated responses for each perturbed question.
        """
        perb_responses = []  # List to store generated responses for perturbed questions
        
        # Generate responses for each perturbed question
        for question in perb_questions:
            prompt = self.prompt_temp + '\nQ:' + question  # Construct the perturbed question prompt

            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                # Call OpenAI API for GPT models
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                # Call Guanaco-33B model API
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                # Call Falcon-7B model API
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")

            # Append each generated response to the list
            perb_responses.append(res)
  
        return perb_responses
