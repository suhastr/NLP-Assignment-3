from sac3 import llm_models
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures

class Evaluate:
    """
    A class to evaluate responses generated by language models (LLMs).
    Supports parallelized evaluation for efficient processing of multiple responses.
    """

    def __init__(self, model):
        """
        Initialize the Evaluate class.

        Args:
            model (str): The name or configuration of the language model to use for evaluation.
        """
        self.model = model
        
        # Template prompt to guide LLM in generating answers.
        self.prompt_temp = 'Please answer the following question:\n'
        
    def openai_api_parallel(self, prompt, temperature):
        """
        Call the OpenAI API to generate a response for a given prompt.

        Args:
            prompt (str): The input prompt/question for the model.
            temperature (float): Sampling temperature to control randomness in responses.

        Returns:
            str: The generated response from the LLM.
        """
        return llm_models.call_openai_model(prompt, self.model, temperature)
    
    def self_evaluate_api(self, self_question, temperature, self_num):
        """
        Generate multiple responses to a single question using parallelized API calls.

        Args:
            self_question (str): The original user query to evaluate.
            temperature (float): Sampling temperature for response generation.
            self_num (int): Number of responses to generate for the question.

        Returns:
            list: Generated responses from the LLM for the given question.
        """
        prompt = self.prompt_temp + self_question  # Construct the prompt
        self_responses = []  # List to store generated responses
        
        # Use ThreadPoolExecutor to parallelize API calls
        with ThreadPoolExecutor(max_workers=self_num) as executor:
            # Submit API calls as separate tasks
            futures = [executor.submit(self.openai_api_parallel, prompt, temperature) for _ in range(self_num)]
            
            # Collect results as tasks complete
            for future in concurrent.futures.as_completed(futures):
                self_responses.append(future.result())

        return self_responses

    def self_evaluate(self, self_question, temperature, self_num):
        """
        Generate multiple responses to a single question sequentially.

        Args:
            self_question (str): The original user query to evaluate.
            temperature (float): Sampling temperature for response generation.
            self_num (int): Number of responses to generate for the question.

        Returns:
            list: Generated responses from the LLM for the given question.
        """
        self_responses = []  # List to store generated responses
        prompt = self.prompt_temp + '\nQ:' + self_question  # Construct the prompt

        # Sequentially generate responses
        for i in range(self_num): 
            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                # Call OpenAI API for GPT models
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                # Call Guanaco-33B model API
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                # Call Falcon-7B model API
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")

            # Append each response to the list
            self_responses.append(res)

        return self_responses

    def perb_evaluate(self, perb_questions, temperature):
        """
        Generate responses to perturbed (semantically equivalent) questions sequentially.

        Args:
            perb_questions (list): List of perturbed questions.
            temperature (float): Sampling temperature for response generation.

        Returns:
            list: Generated responses for each perturbed question.
        """
        perb_responses = []  # List to store generated responses
        
        # Sequentially generate responses for each perturbed question
        for question in perb_questions:
            prompt = self.prompt_temp + '\nQ:' + question  # Construct the prompt

            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                # Call OpenAI API for GPT models
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                # Call Guanaco-33B model API
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                # Call Falcon-7B model API
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")

            # Append each response to the list
            perb_responses.append(res)

        return perb_responses
