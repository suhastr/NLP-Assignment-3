from sac3 import llm_models

class SemanticConsistencyCheck:
    """
    A class to perform semantic consistency checks for hallucination detection 
    in responses generated by large language models (LLMs).
    """

    def __init__(self, model):
        """
        Initialize the SemanticConsistencyCheck class.

        Args:
            model (str): The name or configuration of the language model to use for evaluation.
        """
        self.model = model
        
        # Template prompt for evaluating semantic equivalence between two QA pairs.
        self.prompt_temp = """
        Are the following two Question-Answer(QA) pairs semantically equivalent? 
        Provide your best guess and the probability that it is correct (0.0 to 1.0).
        Given ONLY the guess (Yes or No) and probability, no other words or explanation. 
        For example:
        Guess: <most likely guess, as short as possible; not a complete sentence, just the guess!> 
        Probability: <the probability between 0.0 and 1.0 that your guess is correct, without any extra commentary whatsoever; 
        just the probability!>
        """

    def score_scc(self, question, target_answer, candidate_answers, temperature):
        """
        Compute the semantic consistency score between a target QA pair and multiple candidate QA pairs.

        This function compares the original question-answer pair (target) with several candidate 
        question-answer pairs, evaluating their semantic equivalence using the provided LLM.

        Args:
            question (str): The original user query.
            target_answer (str): The primary response to the query (baseline response).
            candidate_answers (list): List of candidate responses (e.g., from perturbed questions).
            temperature (float): Sampling temperature for response generation (controls randomness).

        Returns:
            tuple:
                - score (float): Average inconsistency score (hallucination metric).
                - sc_output (list): Binary consistency results for each candidate (0 = consistent, 1 = inconsistent).
        """
        if target_answer is None:
            # Target answer is required to perform comparison; raise error if missing.
            raise ValueError("Target answer cannot be None.")

        sc_output = []  # List to store binary consistency values for each candidate answer
        target_pair = f'Q:{question}\nA:{target_answer}'  # Format the target QA pair
        
        num_candidate_answer = len(candidate_answers)
        
        for i in range(num_candidate_answer): 
            # Format the candidate QA pair for comparison
            candidate_pair = f'Q:{question}\nA:{candidate_answers[i]}'
            
            # Construct the prompt to compare the target QA pair with the candidate QA pair
            prompt = (self.prompt_temp + 
                      '\nThe first QA pair is:\n' + target_pair + 
                      '\nThe second QA pair is:\n' + candidate_pair)
            
            # Call the LLM API to evaluate semantic equivalence
            res = llm_models.call_openai_model(prompt, self.model, temperature)
            
            # Parse the response to extract the "Guess" (Yes/No)
            guess = res.split(':')[1].split('\n')[0].strip()
            
            # Map the guess to binary values: 'Yes' = 0 (consistent), 'No' = 1 (inconsistent)
            value = 0 if guess == 'Yes' else 1
            
            # Store the binary result
            sc_output.append(value)
        
        # Compute the inconsistency score as the average of binary values
        score = sum(sc_output) / num_candidate_answer
        
        return score, sc_output
