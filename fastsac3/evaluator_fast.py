import llm_models
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures

class Evaluate:
    """
    A class to evaluate responses generated by language models (LLMs) for hallucination detection 
    tasks. Includes methods for generating responses to original and perturbed questions, both 
    sequentially and in parallel.
    """

    def __init__(self, model):
        """
        Initialize the Evaluate class.

        Args:
            model (str): The name or configuration of the language model to be used for evaluation.
        """
        self.model = model
        
        # Template prompt to guide LLM in generating answers.
        self.prompt_temp = 'Please answer the following question:\n'

    def openai_api_parallel(self, prompt, temperature):
        """
        Call the OpenAI API for generating responses in a parallelized manner.

        Args:
            prompt (str): The input prompt to be passed to the LLM.
            temperature (float): Sampling temperature to control randomness in LLM responses.

        Returns:
            str: The response from the LLM API.
        """
        # Call the LLM API with the given prompt and model configuration.
        return llm_models.call_openai_model(prompt, self.model, temperature)

    def self_evaluate_api(self, self_question, temperature, self_num):
        """
        Generate multiple responses to a single question using parallel API calls.

        Args:
            self_question (str): The original user query to be evaluated.
            temperature (float): Sampling temperature for response generation.
            self_num (int): Number of responses to generate for the question.

        Returns:
            list: List of responses generated by the LLM for the given question.
        """
        prompt = self.prompt_temp + self_question
        self_responses = []
        
        # Use a ThreadPoolExecutor to parallelize LLM API calls.
        with ThreadPoolExecutor(max_workers=self_num + 2) as executor:
            # Submit multiple API calls for parallel execution.
            futures = [executor.submit(self.openai_api_parallel, prompt, temperature) for _ in range(self_num)]
            
            # Collect results from completed threads.
            for future in concurrent.futures.as_completed(futures):
                self_responses.append(future.result())

        return self_responses

    def self_evaluate(self, self_question, temperature, self_num):
        """
        Generate multiple responses to a single question sequentially.

        Args:
            self_question (str): The original user query to be evaluated.
            temperature (float): Sampling temperature for response generation.
            self_num (int): Number of responses to generate for the question.

        Returns:
            list: List of responses generated by the LLM for the given question.
        """
        self_responses = []
        prompt = self.prompt_temp + '\nQ:' + self_question

        # Sequentially generate responses based on the specified LLM.
        for i in range(self_num):
            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")
            
            self_responses.append(res)

        return self_responses

    def perb_evaluate(self, perb_questions, temperature):
        """
        Generate responses to perturbed (semantically equivalent) questions sequentially.

        Args:
            perb_questions (list): List of semantically equivalent perturbed questions.
            temperature (float): Sampling temperature for response generation.

        Returns:
            list: List of responses generated by the LLM for each perturbed question.
        """
        perb_responses = []

        # Sequentially generate responses for each perturbed question.
        for i in range(len(perb_questions)):
            prompt = self.prompt_temp + '\nQ:' + perb_questions[i]

            # Generate response based on the specified LLM.
            if self.model in ['gpt-3.5-turbo', 'gpt-4']:
                res = llm_models.call_openai_model(prompt, self.model, temperature)
            elif self.model == 'guanaco-33b':
                res = llm_models.call_guanaco_33b(prompt, max_new_tokens=200)
            elif self.model == 'falcon-7b':
                res = llm_models.call_falcon_7b(prompt, max_new_tokens=200)
            else:
                raise ValueError(f"Unsupported model: {self.model}")

            perb_responses.append(res)

        return perb_responses
